# -*- coding: utf-8 -*-
"""ROC

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1iUphbx-vczd8G8FeWw56aPYIoU631U
"""

!pip install scikit-learn
!pip install pandas

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score,  confusion_matrix, accuracy_score

df = pd.read_excel('/content/BertScore ROC.xlsx')

y_true = df['TrueLabels'].map({'Yes': 1, 'No': 0}).tolist()
print("True Labels:", y_true)

y_scores = df['Scores'].tolist()
print("Scores:", y_scores)

y_scores = [score if score != float('inf') else max(y_scores) + 1 for score in y_scores]

fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

auc_score = roc_auc_score(y_true, y_scores)
print("AUC Score:", auc_score)

print("True Positive Rate:", tpr)
print("False Positive Rate:", fpr)

optimal_index = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_index]
print("The index of the optimal threshold is =", optimal_index)
print("The optimal threshold value is =", optimal_threshold)

y_pred = (y_scores >= optimal_threshold).astype(int)

# Calculate precision, recall, and F1-score
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
accuracy = accuracy_score(y_true, y_pred)

print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1-Score: {f1:.2f}')
print(f'Accuracy: {accuracy:.2f}')

import seaborn as sns

conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted No', 'Predicted Yes'], yticklabels=['Actual No', 'Actual Yes'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""## Combine score calculation"""

lexical_scores_df = pd.read_excel('/content/Lexical ROC.xlsx')
bertscores_df = pd.read_excel('/content/BertScore ROC.xlsx')

lexical_scores = lexical_scores_df['Scores'].values
bertscores = bertscores_df['Scores'].values

labels = lexical_scores_df['TrueLabels'].values

auc_lexical = 0.7446808510638299
auc_bert = 0.7145390070921985

total_auc = auc_lexical + auc_bert

weight_lexical = auc_lexical / total_auc
weight_bert = auc_bert / total_auc

print(f'Weights: Lexical: {weight_lexical}, BERT: {weight_bert}')

combined_scores = (weight_lexical * lexical_scores) + (weight_bert * bertscores)

labels_numeric = (labels == 'Yes').astype(int)

fpr, tpr, thresholds = roc_curve(labels_numeric, combined_scores)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Combined Scores')
plt.legend(loc="lower right")
plt.show()

optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]

print(f'Optimal Threshold for Combined Score: {optimal_threshold}')