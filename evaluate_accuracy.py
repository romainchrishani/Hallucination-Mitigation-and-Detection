# -*- coding: utf-8 -*-
"""Evaluate accuracy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11kHtxO9GzhXmU2QaPiuHJrfBB5oMA2Y9
"""

import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support


try:
    file_path = '/content/FactCC_BertScore_Evaluation.xlsx'
    df = pd.read_excel(file_path)
except Exception as e:
    print(f"Error loading the file: {e}")
    exit()

if 'probabilities' not in df.columns or 'labels' not in df.columns:
    print("Required columns 'probabilities' and 'labels' not found in the file.")
    exit()

try:

    df['probabilities'] = df['probabilities'].str.replace('[', '').str.replace(']', '').str.split(', ')

    df[['contradiction', 'neutral', 'entailment']] = pd.DataFrame(df['probabilities'].tolist(), index=df.index).astype(float)

    df['predicted'] = df[['contradiction', 'neutral', 'entailment']].idxmax(axis=1)

    df['predicted'] = df['predicted'].map({'contradiction': 0, 'neutral': 1, 'entailment': 1})

    df['labels'] = df['labels'].map({'No': 0, 'Yes': 1})

    true_labels = df['labels']
    predicted_labels = df['predicted']

    cm = confusion_matrix(true_labels, predicted_labels)
    print("Confusion Matrix:")
    print(cm)

    accuracy = accuracy_score(true_labels, predicted_labels)
    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')

    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")

except Exception as e:
    print(f"Error processing the data: {e}")
    exit()

"""## Accuracy and other"""

import pandas as pd
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, accuracy_score
import matplotlib.pyplot as plt

file_path = '/content/Lexical Matrix.xlsx'
df = pd.read_excel(file_path)

model_evaluation = df['model evaluation']
manual_evaluation = df['manual evaluation']

model_evaluation = model_evaluation.map({'Lexically Hallucinated': 0, 'Not Lexically Hallucinated': 1})
manual_evaluation = manual_evaluation.map({'No': 0, 'Yes': 1})

cm = confusion_matrix(manual_evaluation, model_evaluation)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No', 'Yes'])
disp.plot(cmap=plt.cm.Blues)
plt.show()

precision = precision_score(manual_evaluation, model_evaluation)
recall = recall_score(manual_evaluation, model_evaluation)
f1 = f1_score(manual_evaluation, model_evaluation)
accuracy = accuracy_score(manual_evaluation, model_evaluation)

print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')
print(f'Accuracy: {accuracy:.2f}')

from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(manual_evaluation, model_evaluation)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt

# Combined hypothetical data
temperatures = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
hallucination_rates = [1, 2, 3, 4, 6, 9, 15, 20, 40, 60]

# Plotting
plt.figure(figsize=(10, 6))

plt.plot(temperatures, hallucination_rates, marker='o', linestyle='-', color='b', label='General Trend')

# Adding title and labels
plt.title('Hallucination Percentage vs. Temperature')
plt.xlabel('Temperature')
plt.ylabel('Hallucination Rate (%)')
plt.grid(True)
plt.legend()

# Show the plot
plt.show()