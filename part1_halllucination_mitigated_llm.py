# -*- coding: utf-8 -*-
"""Part1-Halllucination mitigated LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Enjmme5DVPMuq0JNjedV5mWxY3iZMGk
"""

!pip install langchain
!pip install sentence-transformers
!pip install faiss-cpu
!pip install tiktoken
!pip install huggingface-hub
!pip install pypdf
!pip install -U langchain-community

from langchain.chains import ConversationalRetrievalChain
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import LlamaCpp
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.document_loaders import PyPDFLoader
import os
import tempfile
import openpyxl
from openpyxl import Workbook,load_workbook
from langchain.llms import DeepInfra
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
import spacy
import re
from langchain.schema import Document

def initialize_session_state():
    session_state = {
        "history": [],
        "generated": ["Hello! Ask me anything about ðŸ¤–"],
        "past": ["Hey! ðŸ‘‹"]
    }
    return session_state

def conversation_chat(query, chain, session_state):
    #session_state["history"] = []
    session_state = initialize_session_state()
    result = chain({"question": query, "chat_history": session_state["history"]})
    session_state["history"].append((query, result["answer"]))
    return result["answer"]

def clean_text(text):
    text = text.page_content
    text = text.replace('\n', ' ')
    text = re.sub(r'\s{2,}', '. ', text)
    text = re.sub(r'(\d)([A-Za-z])', r'\1. \2', text)
    text = re.sub(r'\.(\w)', r'. \1', text)
    text = re.sub(r'(\w)([A-Z])', r'\1. \2', text)
    text = re.sub(r'\s+', ' ', text)
    return Document(page_content=text.strip())

def display_chat_history(chain, session_state, questions):
    workbook = Workbook()
    sheet = workbook.active
    sheet.append(["Question", "Answer"])

    for question, repeats in questions:
        for _ in range(repeats):
            output = conversation_chat(question, chain, session_state)

            sheet.append([question, output])
            print(f"User: {question}\nBot: {output}")

    workbook.save("ChatHistory.xlsx")
    print("Chat history saved to ChatHistory.xlsx")

os.environ["DEEPINFRA_API_TOKEN"] = "wUp54XhDTB1htN0ios9N8JMSg9rKpedN"

def create_conversational_chain(vector_store):
    local_llm = DeepInfra(model_id="mistralai/Mistral-7B-Instruct-v0.1")
    local_llm.model_kwargs = {
        "streaming": True,
        "temperature": 0.5,
        "top_p": 0.6,
        "verbose": True,
        "n_ctx": 4096,
        "num_return_sequences":1,
    }
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    chain = ConversationalRetrievalChain.from_llm(
        llm=local_llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
        memory=memory
    )
    return chain

def main():
    session_state = initialize_session_state()
    print("ChatBot using Mistral-7B-Instruct LLM :books:")

    uploaded_file = '/content/2023.pdf'

    if uploaded_file:
        text = []
        with open(uploaded_file, "rb") as f:
            file_contents = f.read()
        file_extension = os.path.splitext(uploaded_file)[1]
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            temp_file.write(file_contents)
            temp_file_path = temp_file.name
            print("loading: ", uploaded_file)
        loader = None
        if file_extension == ".pdf":
            loader = PyPDFLoader(temp_file_path)

        if loader:
            raw_text = loader.load()
            clean_texts = [clean_text(page) for page in raw_text]
            text.extend(clean_texts)
            os.remove(temp_file_path)

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=10000, chunk_overlap=20
        )

        text_chunks = text_splitter.split_documents(text)

        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={"device": "cpu"},
        )

        vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)

        chain = create_conversational_chain(vector_store)
        questions = []
        while True:
            user_input = input("Enter your question (or type 'done' to finish): ")
            if user_input.lower() == 'done':
                break
            repeats = int(input(f"How many times to repeat '{user_input}': "))
            questions.append((user_input, repeats))

        display_chat_history(chain, session_state, questions)


if __name__ == "__main__":
    main()